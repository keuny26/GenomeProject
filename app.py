import streamlit as st
import fitz  # PyMuPDF
import json
import re
import google.generativeai as genai
from streamlit_agraph import agraph, Node, Edge, Config

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="GenomeGraph AI", layout="wide")
st.title("ğŸ§¬ GenomeGraph AI (Smart Integration)")

# --- API í‚¤ ì„¤ì • ---
if "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]
else:
    st.sidebar.title("ì„¤ì •")
    api_key = st.sidebar.text_input("Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# --- ëª¨ë¸ ì´ˆê¸°í™” ---
model = None
if api_key:
    try:
        genai.configure(api_key=api_key)
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        target_model = 'gemini-1.5-flash'
        if f'models/{target_model}' in available_models:
            model = genai.GenerativeModel(target_model)
        elif available_models:
            model = genai.GenerativeModel(available_models[0].replace('models/', ''))
        
        if model:
            st.sidebar.success(f"ì—°ê²°ë¨: {model.model_name}")
    except Exception as e:
        st.error(f"API ì„¤ì • ì˜¤ë¥˜: {e}")

# --- ë¶„ì„ í•¨ìˆ˜ ---
def analyze_graph_with_ai(text):
    if not model: return None
    prompt = f"""
    ë‹¹ì‹ ì€ ìœ ì „ì²´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì „ìì™€ ì§ˆí™˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
    ë°˜ë“œì‹œ 'nodes'ì™€ 'links' í‚¤ë¥¼ í¬í•¨í•˜ê³ , ê° ë…¸ë“œì—ëŠ” 'id', 'label', 'type', 'desc' í•„ë“œë¥¼ í¬í•¨í•˜ì„¸ìš”.
    
    í˜•ì‹ ì˜ˆì‹œ:
    {{
      "nodes": [{{ "id": "G1", "label": "BRCA1", "type": "gene", "desc": "ìœ ë°©ì•” ìœ„í—˜ ì¦ê°€ì™€ ê´€ë ¨ëœ ìœ ì „ì" }}],
      "links": [{{ "source": "G1", "target": "D1" }}]
    }}
    
    í…ìŠ¤íŠ¸: {text[:15000]}
    """
    try:
        response = model.generate_content(prompt)
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        return None
    except Exception as e:
        st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- ë©”ì¸ UI: ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ---
uploaded_files = st.file_uploader("PDF ë³´ê³ ì„œë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

# ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì í•˜ì—¬ ë³€ê²½ ê°ì§€
current_file_names = [f.name for f in uploaded_files] if uploaded_files else []

if uploaded_files and api_key:
    # íŒŒì¼ êµ¬ì„±ì´ ë°”ë€Œë©´(ìƒˆ íŒŒì¼ ì¶”ê°€/ì‚­ì œ) ì„¸ì…˜ ê°•ì œ ì´ˆê¸°í™” ë° ì¬ë¶„ì„
    if "last_files" not in st.session_state or st.session_state.last_files != current_file_names:
        with st.spinner("ìƒˆë¡œìš´ íŒŒì¼ì„ í¬í•¨í•˜ì—¬ í†µí•© ë¶„ì„ ì¤‘..."):
            combined_text = ""
            for uploaded_file in uploaded_files:
                doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                combined_text += f"\n[Document: {uploaded_file.name}]\n"
                combined_text += " ".join([page.get_text() for page in doc])
            
            st.session_state.full_text = combined_text
            st.session_state.last_files = current_file_names  # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            st.session_state.graph_data = analyze_graph_with_ai(st.session_state.full_text)
            st.session_state.messages = []

    # 2. ê·¸ë˜í”„ ì˜ì—­
    if st.session_state.get("graph_data"):
        st.subheader("ğŸ§¬ í†µí•© ì§€ì‹ ê·¸ë˜í”„")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # --- KeyError ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ë…¸ë“œ/ê°„ì„  ìƒì„± ---
            nodes = []
            raw_nodes = st.session_state.graph_data.get('nodes', [])
            for n in raw_nodes:
                if 'id' in n:
                    # labelì´ ì—†ìœ¼ë©´ idë¥¼ ëŒ€ì‹  ì‚¬ìš©, descê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                    l_text = n.get('label', n['id'])
                    n_type = n.get('type', 'gene')
                    n_color = '#4285F4' if n_type == 'gene' else '#EA4335'
                    nodes.append(Node(id=n['id'], label=l_text, size=25, color=n_color))
            
            edges = []
            raw_links = st.session_state.graph_data.get('links', [])
            for l in raw_links:
                if 'source' in l and 'target' in l:
                    edges.append(Edge(source=l['source'], target=l['target']))

            # ë…¸ë“œê°€ ì¡´ì¬í•  ë•Œë§Œ ê·¸ë˜í”„ ë Œë”ë§
            if nodes:
                config = Config(
                    width=900, 
                    height=600, 
                    directed=True, 
                    physics=True, 
                    hierarchical=False,
                    fit_view=True  # ê·¸ë˜í”„ë¥¼ í™”ë©´ ì¤‘ì•™ì— ìë™ìœ¼ë¡œ ë§ì¶¤
                )
                selected_id = agraph(nodes=nodes, edges=edges, config=config)
            else:
                st.warning("ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                selected_id = None

        with col2:
            st.markdown("### ğŸ” ìƒì„¸ ì •ë³´")
            if selected_id:
                # ì„ íƒëœ ë…¸ë“œì˜ ìƒì„¸ ì •ë³´ ì°¾ê¸°
                node_detail = next((n for n in st.session_state.graph_data.get('nodes', []) if str(n.get('id')) == str(selected_id)), None)
                if node_detail:
                    st.success(f"**ëª…ì¹­:** {node_detail.get('label', selected_id)}")
                    st.info(f"**ì„¤ëª…:** {node_detail.get('desc', 'ì„¤ëª… ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}")
                else:
                    st.write("ìƒì„¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.write("ğŸ’¡ ê·¸ë˜í”„ì˜ ë…¸ë“œë¥¼ í´ë¦­í•˜ë©´ ìƒì„¸ ì •ë³´ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")

    st.divider()

    # 3. ì±„íŒ… ì˜ì—­
    st.subheader("ğŸ’¬ í†µí•© ë¶„ì„ ì±„íŒ…")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                try:
                    # ë¬¸ì„œ ë‚´ìš©ê³¼ í•¨ê»˜ ì§ˆë¬¸ ë˜ì§€ê¸°
                    response = model.generate_content(f"ë¬¸ì„œ í†µí•©ë³¸ ë‚´ìš©: {st.session_state.full_text[:8000]}\n\nì§ˆë¬¸: {prompt}")
                    st.markdown(response.text)
                    st.session_state.messages.append({"role": "assistant", "content": response.text})
                except Exception as e:
                    st.error(f"ì±„íŒ… ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")