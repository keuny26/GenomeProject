import streamlit as st
import fitz  # PyMuPDF
import json
import re
import time
import google.generativeai as genai
from streamlit_agraph import agraph, Node, Edge, Config
from Bio import Entrez  # NCBI ì—°ë™ìš©

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="GenomeGraph AI", layout="wide")
st.title("ğŸ§¬ GenomeGraph AI (Final Stable Version)")

# --- 2. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state: st.session_state.messages = []
if "full_text" not in st.session_state: st.session_state.full_text = ""
if "graph_data" not in st.session_state: st.session_state.graph_data = None

# --- 3. API í‚¤ ë° ì„¤ì • (ì‚¬ì´ë“œë°”) ---
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì • ë° ë³´ì•ˆ")
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.info("âœ… Secretsì—ì„œ API í‚¤ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        api_key = st.text_input("Gemini API Key", type="password", help="í‚¤ëŠ” ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì‚­ì œë©ë‹ˆë‹¤.")
    
    ncbi_email = st.text_input("NCBI ì—°ë™ìš© ì´ë©”ì¼", value="your_email@example.com")
    
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

# --- 4. ëª¨ë¸ ë° NCBI í•¨ìˆ˜ ---
model = None
if api_key:
    try:
        genai.configure(api_key=api_key)
        # ì•ˆì •ì„±ì„ ìœ„í•´ ìµœì‹  flash ëª¨ë¸ ì§€ì •
        model = genai.GenerativeModel('gemini-1.5-flash')
        st.sidebar.success("ëª¨ë¸ ì—°ê²°ë¨: gemini-1.5-flash")
    except Exception as e:
        st.error(f"ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜: {e}")

def get_ncbi_gene_info(gene_name, email):
    Entrez.email = email
    try:
        search_handle = Entrez.esearch(db="gene", term=f"{gene_name}[Gene Name] AND human[Organism]")
        search_results = Entrez.read(search_handle)
        if not search_results["IdList"]: return "NCBI ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        gene_id = search_results["IdList"][0]
        summary_handle = Entrez.esummary(db="gene", id=gene_id)
        summary_record = Entrez.read(summary_handle)
        return summary_record['DocumentSummarySet']['DocumentSummary'][0]['Description']
    except: return "NCBI ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"

# --- 5. ë¶„ì„ ë° ë³‘í•© ë¡œì§ ---
def analyze_single_doc(text, filename):
    if not model: return None
    # ë³´ì•ˆ: ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
    clean_text = re.sub(r'\d{3}-\d{4}-\d{4}', "[PROTECTED]", text)
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ìœ ì „ì²´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì „ì, ì§ˆí™˜(ì¦ìƒ í¬í•¨), ë³€ì´ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    - ì¦ìƒ/ì§ˆë³‘ì€ ë°˜ë“œì‹œ 'type': 'Disease'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    - ì¶œë ¥ í˜•ì‹: {{"nodes": [{{"id": "ID", "label": "ì´ë¦„", "type": "Gene/Disease/Variant/Drug", "desc": "ì„¤ëª…"}}], "links": [{{"source": "ID", "target": "ID"}}]}}
    í…ìŠ¤íŠ¸: {clean_text[:10000]}
    """
    try:
        time.sleep(1) # API í• ë‹¹ëŸ‰ ì¡°ì ˆ
        response = model.generate_content(prompt)
        json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            # ë…¸ë“œë³„ë¡œ ì¶œì²˜ íŒŒì¼ ì •ë³´ ê¸°ë¡
            for n in data['nodes']: n['source_file'] = filename
            return data
        return None
    except Exception as e:
        st.error(f"{filename} ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None

def merge_graphs(results):
    merged_nodes = {}
    merged_links = []
    for data in results:
        if not data: continue
        for n in data['nodes']:
            nid = n['id']
            if nid in merged_nodes:
                # ì—¬ëŸ¬ íŒŒì¼ì—ì„œ ë°œê²¬ëœ ë…¸ë“œëŠ” 'Common'ìœ¼ë¡œ í‘œì‹œ
                merged_nodes[nid]['source_file'] = "Common"
            else:
                merged_nodes[nid] = n
        merged_links.extend(data['links'])
    
    # ì¤‘ë³µ ë§í¬ ì œê±°
    unique_links = [dict(t) for t in {tuple(sorted(d.items())) for d in merged_links}]
    return {"nodes": list(merged_nodes.values()), "links": unique_links}

# --- 6. UI: íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ---
uploaded_files = st.file_uploader("PDF ë³´ê³ ì„œë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

if uploaded_files and api_key:
    if st.button("ğŸ§¬ íŒŒì¼ í†µí•© ë¶„ì„ ì‹œì‘ (ëˆ„ë½ ë°©ì§€ ëª¨ë“œ)"):
        all_results = []
        with st.spinner("ë¬¸ì„œë³„ë¡œ ê°œë³„ ë¶„ì„ì„ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
            full_text_accumulator = ""
            for uploaded_file in uploaded_files:
                doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                text = " ".join([page.get_text() for page in doc])
                full_text_accumulator += f"\n\n[Document: {uploaded_file.name}]\n{text}"
                
                # ë¬¸ì„œë³„ ê°œë³„ ë¶„ì„ ì‹¤í–‰
                res = analyze_single_doc(text, uploaded_file.name)
                all_results.append(res)
            
            st.session_state.full_text = full_text_accumulator
            st.session_state.graph_data = merge_graphs(all_results)
            st.session_state.messages = []
            st.success("ëª¨ë“  ë¬¸ì„œ ë¶„ì„ ë° í†µí•© ì™„ë£Œ!")

    # --- 7. ê·¸ë˜í”„ ì‹œê°í™” ë° í•„í„° ì˜ì—­ ---
    if st.session_state.graph_data:
        st.sidebar.divider()
        st.sidebar.subheader("ğŸ” í•„í„°ë§")
        all_types = list(set([n.get('type', 'Unknown') for n in st.session_state.graph_data['nodes']]))
        selected_types = st.sidebar.multiselect("ë…¸ë“œ íƒ€ì…", all_types, default=all_types)
        search_query = st.sidebar.text_input("ğŸ¯ ë…¸ë“œ ê²€ìƒ‰")

        col1, col2 = st.columns([3, 1])
        
        # NameError ë°©ì§€ë¥¼ ìœ„í•´ selected_id ì´ˆê¸°í™”
        selected_id = None
        
        # íŒŒì¼ë³„ ìƒ‰ìƒ ì„¤ì •
        file_names = [f.name for f in uploaded_files]
        color_palette = ["#4285F4", "#34A853", "#FBBC05", "#8E44AD", "#F39C12", "#16A085"]
        color_map = {name: color_palette[i % len(color_palette)] for i, name in enumerate(file_names)}
        color_map["Common"] = "#EA4335" 

        with col1:
            filtered_nodes = []
            filtered_node_ids = set()
            for n in st.session_state.graph_data['nodes']:
                type_match = n.get('type') in selected_types
                search_match = search_query.lower() in n.get('label', '').lower()
                
                if type_match and search_match:
                    src = n.get('source_file', 'Unknown')
                    n_color = color_map.get(src, "#999999")
                    is_common = src == "Common"
                    filtered_nodes.append(Node(id=n['id'], 
                                               label=f"â­ {n['label']}" if is_common else n['label'], 
                                               size=35 if is_common else 25, 
                                               color=n_color))
                    filtered_node_ids.add(n['id'])
            
            filtered_edges = [Edge(source=l['source'], target=l['target']) 
                              for l in st.session_state.graph_data['links'] 
                              if l['source'] in filtered_node_ids and l['target'] in filtered_node_ids]

            if filtered_nodes:
                config = Config(width=900, height=600, directed=True, physics=True, fit_view=True, panAndZoom=True)
                # ì—¬ê¸°ì„œ í´ë¦­ëœ ë…¸ë“œ IDë¥¼ ê°€ì ¸ì˜´
                selected_id = agraph(nodes=filtered_nodes, edges=filtered_edges, config=config)

        with col2:
            st.markdown("### ğŸ¨ ë²”ë¡€ ë° ìƒì„¸ ì •ë³´")
            for src, color in color_map.items():
                st.markdown(f"<span style='color:{color}'>â—</span> **{src}**", unsafe_allow_html=True)
            
            st.divider()
            # selected_idê°€ Noneì´ ì•„ë‹ ë•Œë§Œ ìƒì„¸ ì •ë³´ ì¶œë ¥ (NameError ë°©ì§€)
            if selected_id:
                node_detail = next((n for n in st.session_state.graph_data['nodes'] if str(n['id']) == str(selected_id)), None)
                if node_detail:
                    st.success(f"**ëª…ì¹­:** {node_detail['label']} ({node_detail['type']})")
                    st.info(f"**ì¶œì²˜:** {node_detail.get('source_file', 'N/A')}")
                    
                    if node_detail['type'] == "Gene":
                        with st.spinner("NCBI í™•ì¸ ì¤‘..."):
                            ncbi_info = get_ncbi_gene_info(node_detail['label'], ncbi_email)
                            st.caption(f"**NCBI Summary:** {ncbi_info}")
                    
                    st.link_button("ğŸ§¬ NCBI ë°”ë¡œê°€ê¸°", f"https://www.ncbi.nlm.nih.gov/gene/?term={node_detail['label']}")
                    st.write(f"**AI ë¶„ì„ ìƒì„¸:**\n{node_detail.get('desc', 'ì„¤ëª… ì—†ìŒ')}")
            else:
                st.write("ê·¸ë˜í”„ì˜ ë…¸ë“œë¥¼ í´ë¦­í•˜ë©´ ìƒì„¸ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# --- 8. ì±„íŒ… ì˜ì—­ ---
st.divider()
st.subheader("ğŸ’¬ ë°ì´í„° ë³´ì•ˆ ì±„íŒ…")
for message in st.session_state.messages:
    with st.chat_message(message["role"]): st.markdown(message["content"])

if chat_prompt := st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": chat_prompt})
    with st.chat_message("user"): st.markdown(chat_prompt)
    with st.chat_message("assistant"):
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ (8000ì ì œí•œ)
            res = model.generate_content(f"Context: {st.session_state.full_text[:8000]}\nQuestion: {chat_prompt}")
            st.markdown(res.text)
            st.session_state.messages.append({"role": "assistant", "content": res.text})
        except Exception as e: st.error(f"ì±„íŒ… ì˜¤ë¥˜: {e}")