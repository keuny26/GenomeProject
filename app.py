import streamlit as st
import fitz  # PyMuPDF
import json
import re
import google.generativeai as genai
from streamlit_agraph import agraph, Node, Edge, Config

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="GenomeGraph AI", layout="wide")
st.title("ğŸ§¬ GenomeGraph AI (Auto-Model Detection)")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (AttributeError ë°©ì§€) ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "full_text" not in st.session_state:
    st.session_state.full_text = ""
if "graph_data" not in st.session_state:
    st.session_state.graph_data = None

# --- API í‚¤ ì„¤ì • ---
if "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]
else:
    st.sidebar.title("ì„¤ì •")
    api_key = st.sidebar.text_input("Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# --- ëª¨ë¸ ì´ˆê¸°í™” (404 ì—ëŸ¬ ë°©ì§€ìš© ìë™ ê°ì§€) ---
model = None
if api_key:
    try:
        genai.configure(api_key=api_key)
        
        # ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ í™•ì¸
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        
        # ê¸°ë³¸ íƒ€ê²Ÿ ì„¤ì •
        target_model_name = 'models/gemini-1.5-flash'
        
        if target_model_name in available_models:
            model = genai.GenerativeModel('gemini-1.5-flash')
        elif available_models:
            # íƒ€ê²Ÿ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ëª¨ë¸(ë³´í†µ gemini-pro ë“±) ì‚¬ìš©
            fallback = available_models[0].replace('models/', '')
            model = genai.GenerativeModel(fallback)
            st.sidebar.warning(f"Flash ëª¨ë¸ ë¯¸ì§€ì›ìœ¼ë¡œ {fallback} ëª¨ë¸ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if model:
            st.sidebar.success(f"ì—°ê²°ë¨: {model.model_name}")
    except Exception as e:
        st.error(f"API/ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜: {e}")

# --- ë¶„ì„ í•¨ìˆ˜ (ì¶œì²˜ íƒœê¹… í¬í•¨) ---
def analyze_graph_with_ai(text):
    if not model: return None
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ìœ ì „ì²´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì „ìì™€ ì§ˆí™˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
    1. ëª¨ë“  ë…¸ë“œì— 'source_file' í•„ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ì–´ë–¤ ë¬¸ì„œ([Document: íŒŒì¼ëª…]) ì¶œì²˜ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”.
    2. ê³µí†µ ë…¸ë“œëŠ” 'source_file'ì„ "Common"ìœ¼ë¡œ í•˜ì„¸ìš”.
    3. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
    
    ë°ì´í„° êµ¬ì¡° ì˜ˆì‹œ:
    {{
      "nodes": [{{ "id": "G1", "label": "BRCA1", "type": "gene", "source_file": "file1.pdf", "desc": "ì„¤ëª…" }}],
      "links": [{{ "source": "G1", "target": "D1" }}]
    }}

    í…ìŠ¤íŠ¸: {text[:20000]}
    """
    try:
        response = model.generate_content(prompt)
        json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        return None
    except Exception as e:
        st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --- ë©”ì¸ UI: ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ---
uploaded_files = st.file_uploader("PDF ë³´ê³ ì„œë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

if uploaded_files and api_key:
    if st.button("ğŸ§¬ íŒŒì¼ë³„ í†µí•© ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  í†µí•© ê·¸ë˜í”„ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            combined_text = ""
            for uploaded_file in uploaded_files:
                doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                combined_text += f"\n\n[Document: {uploaded_file.name}]\n"
                combined_text += " ".join([page.get_text() for page in doc])
            
            st.session_state.full_text = combined_text
            st.session_state.graph_data = analyze_graph_with_ai(st.session_state.full_text)
            st.session_state.messages = [] 
            st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 2. ê·¸ë˜í”„ ì˜ì—­
    if st.session_state.graph_data:
        st.subheader("ğŸ§¬ ì¶œì²˜ë³„ í†µí•© ì§€ì‹ ê·¸ë˜í”„")
        st.info("ğŸ’¡ ë§ˆìš°ìŠ¤ íœ ë¡œ í™•ëŒ€/ì¶•ì†Œê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë…¸ë“œë¥¼ ìƒì–´ë²„ë¦¬ë©´ ì•„ë˜ ì •ë ¬ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
        
        col1, col2 = st.columns([3, 1])
        
        # íŒŒì¼ë³„ ìƒ‰ìƒ ë§¤í•‘
        file_names = [f.name for f in uploaded_files]
        color_palette = ["#4285F4", "#34A853", "#FBBC05", "#8E44AD", "#F39C12", "#16A085"]
        color_map = {name: color_palette[i % len(color_palette)] for i, name in enumerate(file_names)}
        color_map["Common"] = "#EA4335" 

        with col1:
            nodes = []
            for n in st.session_state.graph_data.get('nodes', []):
                src = n.get('source_file', 'Unknown')
                n_color = color_map.get(src, "#999999")
                n_size = 35 if src == "Common" else 25
                label = f"[{src}] {n.get('label')}" if src != "Common" else f"â­ {n.get('label')}"
                
                nodes.append(Node(id=n['id'], label=label, size=n_size, color=n_color))
            
            edges = [Edge(source=l['source'], target=l['target']) for l in st.session_state.graph_data.get('links', [])]

            if nodes:
                config = Config(
                    width=900, height=600, directed=True, physics=True, 
                    fit_view=True, panAndZoom=True, nodeHighlightBehavior=True, 
                    highlightColor="#F79767"
                )
                selected_id = agraph(nodes=nodes, edges=edges, config=config)
                
                if st.button("ğŸ¯ ê·¸ë˜í”„ ì¤‘ì•™ ì •ë ¬ (Reset View)"):
                    st.rerun()
            else:
                st.warning("ë¶„ì„ ê²°ê³¼ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                selected_id = None

        with col2:
            st.markdown("### ğŸ¨ ë²”ë¡€ ë° ìƒì„¸ ì •ë³´")
            for src, color in color_map.items():
                st.markdown(f"<span style='color:{color}'>â—</span> **{src}**", unsafe_allow_html=True)
            
            st.divider()
            if selected_id:
                node_detail = next((n for n in st.session_state.graph_data['nodes'] if str(n['id']) == str(selected_id)), None)
                if node_detail:
                    st.success(f"**ëª…ì¹­:** {node_detail.get('label')}")
                    st.info(f"**ì¶œì²˜:** {node_detail.get('source_file')}")
                    st.write(f"**ë¶„ì„ ìƒì„¸:**\n{node_detail.get('desc', 'ì„¤ëª… ì—†ìŒ')}")
            else:
                st.write("ğŸ’¡ ë…¸ë“œë¥¼ í´ë¦­í•˜ë©´ AIì˜ ìƒì„¸ ë¶„ì„ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    st.divider()

    # 3. ì±„íŒ… ì˜ì—­
    st.subheader("ğŸ’¬ í†µí•© ë¶„ì„ ì±„íŒ…")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ì´ ìœ ì „ì²´ ë°ì´í„°ë“¤ì˜ ìƒê´€ê´€ê³„ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            try:
                chat_response = model.generate_content(f"ìœ ì „ì²´ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•˜ì„¸ìš”. ì—…ë¡œë“œëœ ëª¨ë“  ë¬¸ì„œì˜ ìš”ì•½ë³¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n{st.session_state.full_text[:8000]}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}")
                st.markdown(chat_response.text)
                st.session_state.messages.append({"role": "assistant", "content": chat_response.text})
            except Exception as e:
                st.error(f"ì±„íŒ… ì‘ë‹µ ì¤‘ ì˜¤ë¥˜: {e}")