import streamlit as st
import fitz  # PyMuPDF
import json
import re
import google.generativeai as genai
from streamlit_agraph import agraph, Node, Edge, Config

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="GenomeGraph AI", layout="wide")
st.title("ğŸ§¬ GenomeGraph AI (Clean Labels)")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (AttributeError ë°©ì§€) ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "full_text" not in st.session_state:
    st.session_state.full_text = ""
if "graph_data" not in st.session_state:
    st.session_state.graph_data = None

# --- API í‚¤ ì„¤ì • ---
if "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]
else:
    st.sidebar.title("ì„¤ì •")
    api_key = st.sidebar.text_input("Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# --- ëª¨ë¸ ì´ˆê¸°í™” (ìë™ ê°ì§€ ë¡œì§) ---
model = None
if api_key:
    try:
        genai.configure(api_key=api_key)
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        target_model_name = 'models/gemini-1.5-flash'
        
        if target_model_name in available_models:
            model = genai.GenerativeModel('gemini-1.5-flash')
        elif available_models:
            fallback = available_models[0].replace('models/', '')
            model = genai.GenerativeModel(fallback)
            st.sidebar.warning(f"Flash ëª¨ë¸ ë¯¸ì§€ì›ìœ¼ë¡œ {fallback} ëª¨ë¸ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if model:
            st.sidebar.success(f"ì—°ê²°ë¨: {model.model_name}")
    except Exception as e:
        st.error(f"API/ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜: {e}")

# --- ë¶„ì„ í•¨ìˆ˜ ---
def analyze_graph_with_ai(text):
    if not model: return None
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ìœ ì „ì²´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì „ìì™€ ì§ˆí™˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
    1. ëª¨ë“  ë…¸ë“œì— 'source_file' í•„ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ì¶œì²˜ë¥¼ ê¸°ë¡í•˜ì„¸ìš”.
    2. ê³µí†µ ë…¸ë“œëŠ” 'source_file'ì„ "Common"ìœ¼ë¡œ í•˜ì„¸ìš”.
    3. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
    í…ìŠ¤íŠ¸: {text[:20000]}
    """
    try:
        response = model.generate_content(prompt)
        json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        return None
    except Exception as e:
        st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --- ë©”ì¸ UI: ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ---
uploaded_files = st.file_uploader("PDF ë³´ê³ ì„œë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

if uploaded_files and api_key:
    if st.button("ğŸ§¬ íŒŒì¼ë³„ í†µí•© ë¶„ì„ ì‹œì‘"):
        with st.spinner("ë°ì´í„° ë¶„ì„ ì¤‘..."):
            combined_text = ""
            for uploaded_file in uploaded_files:
                doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                combined_text += f"\n\n[Document: {uploaded_file.name}]\n"
                combined_text += " ".join([page.get_text() for page in doc])
            
            st.session_state.full_text = combined_text
            st.session_state.graph_data = analyze_graph_with_ai(st.session_state.full_text)
            st.session_state.messages = [] 
            st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 2. ê·¸ë˜í”„ ì˜ì—­
    if st.session_state.graph_data:
        st.subheader("ğŸ§¬ ì¶œì²˜ë³„ í†µí•© ì§€ì‹ ê·¸ë˜í”„")
        col1, col2 = st.columns([3, 1])
        
        file_names = [f.name for f in uploaded_files]
        color_palette = ["#4285F4", "#34A853", "#FBBC05", "#8E44AD", "#F39C12", "#16A085"]
        color_map = {name: color_palette[i % len(color_palette)] for i, name in enumerate(file_names)}
        color_map["Common"] = "#EA4335" 

        with col1:
            nodes = []
            for n in st.session_state.graph_data.get('nodes', []):
                src = n.get('source_file', 'Unknown')
                n_color = color_map.get(src, "#999999")
                n_size = 35 if src == "Common" else 25
                
                # --- âœ… ìˆ˜ì • í¬ì¸íŠ¸: [pdfì´ë¦„] íƒœê·¸ë¥¼ ì‚­ì œí•˜ê³  ìˆœìˆ˜ ì´ë¦„ë§Œ í‘œì‹œ ---
                # ê³µí†µ ë…¸ë“œì¼ ë•Œë§Œ êµ¬ë¶„ì„ ìœ„í•´ ë³„í‘œ(â­)ë¥¼ ìœ ì§€í•˜ê³ , ì•„ë‹ˆë©´ ë ˆì´ë¸” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                clean_label = f"â­ {n.get('label')}" if src == "Common" else n.get('label')
                
                nodes.append(Node(id=n['id'], label=clean_label, size=n_size, color=n_color))
            
            edges = [Edge(source=l['source'], target=l['target']) for l in st.session_state.graph_data.get('links', [])]

            if nodes:
                config = Config(width=900, height=600, directed=True, physics=True, fit_view=True, panAndZoom=True)
                selected_id = agraph(nodes=nodes, edges=edges, config=config)
                
                if st.button("ğŸ¯ ê·¸ë˜í”„ ì¤‘ì•™ ì •ë ¬"):
                    st.rerun()
            else:
                st.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                selected_id = None

        with col2:
            st.markdown("### ğŸ¨ ë²”ë¡€ ë° ìƒì„¸ ì •ë³´")
            for src, color in color_map.items():
                st.markdown(f"<span style='color:{color}'>â—</span> **{src}**", unsafe_allow_html=True)
            
            st.divider()
            if selected_id:
                node_detail = next((n for n in st.session_state.graph_data['nodes'] if str(n['id']) == str(selected_id)), None)
                if node_detail:
                    st.success(f"**ëª…ì¹­:** {node_detail.get('label')}")
                    st.info(f"**ì¶œì²˜:** {node_detail.get('source_file')}")
                    st.write(f"**ë¶„ì„ ìƒì„¸:**\n{node_detail.get('desc', 'ì„¤ëª… ì—†ìŒ')}")

    st.divider()

    # 3. ì±„íŒ… ì˜ì—­
    st.subheader("ğŸ’¬ í†µí•© ë¶„ì„ ì±„íŒ…")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            try:
                res = model.generate_content(f"ë‚´ìš© ìš”ì•½: {st.session_state.full_text[:8000]}\nì§ˆë¬¸: {prompt}")
                st.markdown(res.text)
                st.session_state.messages.append({"role": "assistant", "content": res.text})
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")